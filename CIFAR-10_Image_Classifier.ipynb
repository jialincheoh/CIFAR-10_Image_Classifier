{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, you will build a neural network of your own design to evaluate the CIFAR-10 dataset.\n",
    "\n",
    "To meet the requirements for this project, you will need to achieve an accuracy greater than 45%. \n",
    "If you want to beat Detectocorp's algorithm, you'll need to achieve an accuracy greater than 70%. \n",
    "(Beating Detectocorp's algorithm is not a requirement for passing this project, but you're encouraged to try!)\n",
    "\n",
    "Some of the benchmark results on CIFAR-10 include:\n",
    "\n",
    "78.9% Accuracy | [Deep Belief Networks; Krizhevsky, 2010](https://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf)\n",
    "\n",
    "90.6% Accuracy | [Maxout Networks; Goodfellow et al., 2013](https://arxiv.org/pdf/1302.4389.pdf)\n",
    "\n",
    "96.0% Accuracy | [Wide Residual Networks; Zagoruyko et al., 2016](https://arxiv.org/pdf/1605.07146.pdf)\n",
    "\n",
    "99.0% Accuracy | [GPipe; Huang et al., 2018](https://arxiv.org/pdf/1811.06965.pdf)\n",
    "\n",
    "98.5% Accuracy | [Rethinking Recurrent Neural Networks and other Improvements for ImageClassification; Nguyen et al., 2020](https://arxiv.org/pdf/2007.15161.pdf)\n",
    "\n",
    "Research with this dataset is ongoing. Notably, many of these networks are quite large and quite expensive to train. \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains the essential imports you will need – DO NOT CHANGE THE CONTENTS! ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Specify your transforms as a list first.\n",
    "The transforms module is already loaded as `transforms`.\n",
    "\n",
    "CIFAR-10 is fortunately included in the torchvision module.\n",
    "Then, you can create your dataset using the `CIFAR10` object from `torchvision.datasets` ([the documentation is available here](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)).\n",
    "Make sure to specify `download=True`! \n",
    "\n",
    "Once your dataset is created, you'll also need to define a `DataLoader` from the `torch.utils.data` module for both the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define transforms\n",
    "\n",
    "# For the Normalize method, we have 3 channels, ( red, green, blue ). First tuple represents the mean for the 3 channels, \n",
    "# and the second tuple represents the standard deviation for the 3 channels. \n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandomVerticalFlip(0.5), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        transform=transform, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=5,\n",
    "                                         num_workers=3, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       transform=transform, download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=5,\n",
    "                                         num_workers=3, shuffle=False)\n",
    "\n",
    "\n",
    "# The 10 classes in the dataset\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "Using matplotlib, numpy, and torch, explore the dimensions of your data.\n",
    "\n",
    "You can view images using the `show5` function defined below – it takes a data loader as an argument.\n",
    "Remember that normalized images will look really weird to you! You may want to try changing your transforms to view images.\n",
    "Typically using no transforms other than `toTensor()` works well for viewing – but not as well for training your network.\n",
    "If `show5` doesn't work, go back and check your code for creating your data loaders and your training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show5(img_loader):\n",
    "    dataiter = iter(img_loader)\n",
    "    \n",
    "    batch = next(dataiter)\n",
    "    labels = batch[1][0:5]\n",
    "    images = batch[0][0:5]\n",
    "    for i in range(5):\n",
    "        print(classes[labels[i]])\n",
    "    \n",
    "        image = images[i].numpy()\n",
    "        plt.imshow(np.rot90(image.T, k=3))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "Using the layers in `torch.nn` (which has been imported as `nn`) and the `torch.nn.functional` module (imported as `F`), construct a neural network based on the parameters of the dataset. \n",
    "Feel free to construct a model of any architecture – feedforward, convolutional, or even something more advanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 24, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(24, 34, 3)\n",
    "        self.fc1 = nn.Linear(34 * 6 * 6, 100)\n",
    "        self.fc2 = nn.Linear(100, 80)\n",
    "        self.fc3 = nn.Linear(80, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 34 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the network\n",
    "net = Net()\n",
    "net.cuda()   # Enable GPU operations\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a loss function and an optimizer, and instantiate the model.\n",
    "\n",
    "If you use a less common loss function, please note why you chose that loss function in a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your Neural Network\n",
    "Use whatever method you like to train your neural network, and ensure you record the average loss at each epoch. \n",
    "Don't forget to use `torch.device()` and the `.to()` method for both your model and your data if you are using GPU!\n",
    "\n",
    "If you want to print your loss during each epoch, you can use the `enumerate` function and print the loss after a set number of batches. 250 batches works well for most people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 48 %\n",
      "accuracy: 51 %\n",
      "accuracy: 55 %\n",
      "accuracy: 55 %\n",
      "accuracy: 57 %\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "## Reference - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "## This reference - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html has been very useful for me\n",
    "## Pytorch Tutorial on GitHub https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py \n",
    "## that I used to complete this assignment \n",
    "\n",
    "from torch.autograd import Variable\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    for data in trainloader: \n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    correctly_classified = 0\n",
    "    total_count = 0\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(Variable(images.cuda())).cpu()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_count += labels.size(0)\n",
    "        correctly_classified += (predicted == labels).sum()\n",
    "        \n",
    "    value_accuracy = 100 * correctly_classified / total_count\n",
    "\n",
    "    print('accuracy: %d %%' % value_accuracy)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training loss (and validation loss/accuracy, if recorded)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model\n",
    "Using the previously created `DataLoader` for the test set, compute the percentage of correct predictions using the highest probability prediction. \n",
    "\n",
    "If your accuracy is over 70%, great work! \n",
    "This is a hard task to exceed 70% on.\n",
    "\n",
    "If your accuracy is under 45%, you'll need to make improvements.\n",
    "Go back and check your model architecture, loss function, and optimizer to make sure they're appropriate for an image classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your model\n",
    "Using `torch.save`, save your model for future loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "PATH = './final_cifar.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Recommendation\n",
    "\n",
    "Based on your evaluation, what is your recommendation on whether to build or buy? Explain your reasoning below.\n",
    "\n",
    "Some things to consider as you formulate your recommendation:\n",
    "* How does your model compare to Detectocorp's model?\n",
    "* How does it compare to the far more advanced solutions in the literature? \n",
    "* What did you do to get the accuracy you achieved? \n",
    "* Is it necessary to improve this accuracy? If so, what sort of work would be involved in improving it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference - https://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "**Double click this cell to modify it**\n",
    "\n",
    "- My model doesn't have 70% accuracy, so it is not beating Detectocorp's algorithm. \n",
    "- My model only has a 57% after training on the specified number of epoch, it is not as good as those models mentioned in the literature above, but I think it is doing its job, considering that it is above the 45% accuracy required for this project. If I have the necessary resources such as experience and computational resources, my model would probably do much better. \n",
    "- I have two convolutional layers with a pooling layer and 3 fully-connected layers. The input of my first convolutional layers is 3 since I have 3 RGB layers. 24 is the output and 3 as the filter size. Filter size of my pooling layer would be 2 by 2. My second convolutional layer would have the input of 24, output of 34 and filter size of 3. And then I have the fully connected layer of 34 x 6 x 6. How I get the values for the fully connected layer is the following. Since I am working with 32 by 32 images, \n",
    "\n",
    "Convolutional layer formula is as follow - ( ( height - filter-size + 2 ( padding ) ) / s ) + 1\n",
    "\n",
    "\n",
    "First convolutional layer I have filter size 3, stride = 1, and padding = 0. \n",
    "output is then 32 - 3 / 1 + 1 = 30 -> so we have 30 x 30 x 24 \n",
    "\n",
    "Pooling output => 30 / 2 = 15 , so we have 15 x 15 x 24\n",
    "\n",
    "Second convolutional layer output = ( 15 - 3 ) / 1 + 1 = 13 x 13 x 34\n",
    "\n",
    "Pooling output => 13 / 2 = 6 => 34 x 6 x 6\n",
    "\n",
    "Fully connected layer 2 will be linear from 100 to 80. \n",
    "\n",
    "And then last layer will also be linear from 80 to 10 since we have 10 classes. \n",
    "\n",
    "And then for the forward part, I activate the first convolutional layer with relu function, and then enclose in a pooling layer. \n",
    "\n",
    "Similarly for the second convolutional layer. \n",
    "\n",
    "Then, I convert the 3 channels into one channel with the view function. \n",
    "\n",
    "I then use the relu layer on the fully connected layer, repeat one more time on the second fully connected layer. \n",
    "\n",
    "And then I apply the dropout function before apply the third fully connected layer. \n",
    "\n",
    "\n",
    "- Although my model pass the bare minimum accuracy required for this project, I do recommend improving the accuracy by training over more epochs. Another thing that could be tried is applying an additional dropout layer to make the model more robust and less prone to overfitting. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Your Project\n",
    "\n",
    "When you are finished editing the notebook and are ready to turn it in, simply click the **SUBMIT PROJECT** button in the lower right.\n",
    "\n",
    "Once you submit your project, we'll review your work and give you feedback if there's anything that you need to work on. If you'd like to see the exact points that your reviewer will check for when looking at your work, you can have a look over the project [rubric](https://review.udacity.com/#!/rubrics/3077/view)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
